---
title: "Midterm_Kapuvari"
author: "Richard Barad and Trevor Kapuvari"
date: "2023-09-25"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
    number_sections: yes
editor_options: 
  markdown: 
    wrap: 72
---

## Introduction

In response to the growing demand by Zillow for a housing market model that incorporates local context, we have been tasked with leverage data from multiple sources, including Philadelphia Open Data and the American Community Survey data from U.S Census Bureau to gather comprehensive information that enables us to develop an accurate and reliable model for sales prices in the Philadelphia housing market. 

This report outlines our approach and methods and the importance of understand the local context that is integrated with pre-existing data. We aim to provide greater context and a precise understanding of Philadelphia's housing scene for present and future use. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
library(tidyverse)
library(sf)
library(spdep)
library(caret)
library(ckanr)
library(FNN)
library(viridis)
library(grid)
library(gridExtra)
library(ggcorrplot) # plot correlation plot
library(corrr)      # another way to plot correlation plot
library(kableExtra)
library(jtools)     # for regression model plots
library(ggstance) # to support jtools plots
library(ggpubr)    # plotting R^2 value on ggplot point scatter
library(broom.mixed) # needed for effects plots
library(tidycensus)
library(ggplot2)
library(sp)
library(scales)
library(ggiraph)
library(plotly)
library(stargazer)
palette5 <- c("#ffffcc","#a1dab4","#41b6c4","#2c7fb8","#253494")
options(scipen=999)

source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")
```

# Load House Sales Data from Github

We start by importing the home sales data set which was provided for our study. The dataset includes key information on internal home characteristics, such as number of bathrooms, number of stories, total livable area, and interior condition. We carefully reviewed the data to identify which internal characteritics from the provided dataset could be used in home sales price model. Some home attributes like the presence of central air were incomplete and contained multiple blank values which prevented us from using them in our model. 

Other variables, required cleaning and usage of assumptions to ensure data completeness. The assumptions which were used to replace unknown values are listed below:

* 3,679 homes were listed as having zero bathrooms or did not have data on the number of bathrooms. It was assumed these homes contained one bathroom. 
* 8 homes did not have data on interior condition - an assumption was made that these homes had an average interior condition.  
* 1 home did not have data on exterior condition - an assumption was made that this home had an average interior condition
* 208 homes did not have data on the number of fireplaces - an assumption was made that these homes did not have a fireplace

These usage of assumptions was necessary in order to ensure that every home in the home sales dataset could be utilized in the model.

```{r read_github_data, results = "hide"}
house_data <- st_read("https://raw.githubusercontent.com/mafichman/musa_5080_2023/main/Midterm/data/2023/studentData.geojson") %>%
  dplyr::select("objectid","central_air","exterior_condition", "fireplaces", "garage_spaces", "interior_condition", "mailing_street", "location", "number_of_bathrooms", "number_of_bedrooms","number_of_rooms","number_stories","sale_price","sale_date","total_area","total_livable_area","year_built","toPredict") %>%
  st_transform('EPSG:2272')

#Do some feature enginerring
house_data <- house_data %>%
  mutate(
    #Recode exterior condition so that four is best condition, group some categories together
    exterior_condition = as.numeric(recode(exterior_condition, '7'='1', '6'='2', '5'='3', '4'='3', '3'='4', '2'='4', '1'='4')), 
    #Assumed averager condition is value is NA
    exterior_condition = ifelse(is.na(exterior_condition),2,exterior_condition),
    #Set blank values to 0, assumed that if value is blank there are no fireplaces based on metadata.
    fireplaces = ifelse(is.na(fireplaces),0,fireplaces),
    #Assumed there is always at least one bathroom - if property has 0 bathrooms assigned a value of 1.
    number_of_bathrooms = ifelse(number_of_bathrooms == 0,1,number_of_bathrooms),
    number_of_bathrooms = ifelse(is.na(number_of_bathrooms),1,number_of_bathrooms),
    #Assumed averager condition is value is NA
    interior_condition = ifelse(is.na(interior_condition),4,interior_condition))
```


# Get Other Data

In order to develop an enhanced understanding on the housing market, we acquired data that looks beyond the house itself, we gathered data on the external characteristics including the houses proximity to key amenities like urban corridors, superior public schools, restaurants, green space. We also examined the economic and racial demographics of the geogrhic area each home is located in.

## Census Data

The first source used was from the U.S Census Bureau - specifically the American Community Survey (ACS) dataset - we used data from the 2021 five year ACS survey, which is the most recent ACS five year survey data available. We leveraged Social Explorer - a website which makes census data table variables more accessible to help identify variables which we hypothesized would likely correlate with home sales price. 

After a through review - we selected the following data from ACS for potential inclusion in our housing sales price model. 

* **White Home Ownership Rate**: Calculated as the Number of Homes where the home owner is white divided by the total housing units in the census tract. This variable gives our model a racial element. Race unfortunately continues to be a predictor of home values in Philadelphia due the lasting impacts of redlining. We included white home ownership in our model because we hypothesize that home values will likely be higher in areas where the majority of home owners are white. 

* **Median Household Income**: Median income levels can be an important predictor of home sales prices. We hypothesize that areas where median income levels are high are also likely to have high home sales values.  

* **Percent of Households with Public Assistance Income**: Households receiving public assistance income are likely financial strained. We hypothesize that areas where a large number of households receive public assistance are likely to have lower home sales values. 

* **Percent of Homes which are occupied**: Philadelphia contains many vacant lots and properties when compared to other cities. We hypothesize that census tracts where a large number of homes are not occupied are likely to have lower home values. 

These variables provide a basic overview of demographic and income characteristics of neighborhoods in Philadelphia.

```{r load_key, warning = FALSE, eval = FALSE}
census_api_key("2ad9e737f3d9062836cb46bb568be5467f86d3db", overwrite = TRUE)
```

```{r acs_vars, include=FALSE}

acs_vars <- c("B01001_001E", # ACS total Pop estimate
              "B25001_001E", # Estimate of total housing units
              "B25002_003E", # Number of vacant housing units
              "B19013_001E", # Median HH Income ($)
              "B19058_002E", # Households with Public Assistance Income
              "B25003_002E", # Owner Occupied Housing Units
              "B25006_002E", # Homeowner is White
              "B25006_001E") # Total Occupied Housing Units 
```

```{r get_acs_2021, message=FALSE, warning=FALSE, cache=TRUE, include=FALSE}
acsTractsPHL.2021 <- get_acs(geography = "tract",
                             year = 2021, 
                             variables = acs_vars, 
                             geometry = TRUE, 
                             state = "PA", 
                             county = "Philadelphia", 
                             output = "wide") %>%
  st_transform('EPSG:2272')
```

```{r do_some_dplyr, warning=FALSE, cache=FALSE, include=FALSE}
acsTractsPHL.2021 <- acsTractsPHL.2021 %>%
  dplyr::select (GEOID, NAME, all_of(acs_vars))

acsTractsPHL.2021 <- acsTractsPHL.2021 %>%
  rename (totalPop = B01001_001E,
          totalHU = B25001_001E,
          totalVacant = B25002_003E,
          medHHInc = B19013_001E,
          HHAssistedInc = B19058_002E,
          OwnerOccH = B25003_002E,
          WhiteHomeowner = B25006_002E,
          TotalOccH = B25006_001E) %>%
  dplyr::filter(totalPop != 0)

acsTractsPHL.2021 <- acsTractsPHL.2021 %>%
  mutate(HHOccupiedRate = ifelse(OwnerOccH==0,0,OwnerOccH/totalHU),
         WhiteHOrate = ifelse(WhiteHomeowner==0,0,WhiteHomeowner/TotalOccH),
         AssistedIncRate = ifelse(HHAssistedInc==0,0,HHAssistedInc/totalHU),
         medHHInc = ifelse(is.na(medHHInc),mean(medHHInc,na.rm=TRUE),medHHInc))
```

## Get Data from Open Data Philly

OpenData Philly is an open source website that provides a catalog of free data, officially sponsored by the City of Philadelphia. The data provided by the city and other organizations allows us to collect a wide variety of information we can utilize to categorize, describe, and develop a profile for key geographic characteristics which might impact price.

The code below downloads the following datasets:

* [Planning Districts](https://opendataphilly.org/datasets/planning-districts/) - planning districts will be used as a proxy for neighborhoods.

* [Redevelopment Areas](https://opendataphilly.org/datasets/redevelopment-certified-areas/) - this dataset provides information on blighted areas, blighted areas are defined by the city as "meeting one of seven city mandated criteria, including unsafe, unsanitary and inadequate conditions; economically or socially undesirable land use; and faulty street and lot layout". Our assumption is that prices are likely to be lower in blighted areas.

* [Total Restaurants](https://opendataphilly.org/datasets/neighborhood-food-retail/) - this dataset includes information on the number restaurants per census block. The dataset was created from the Neighborhood Food Retail in Philadelphia report. We include this dataset because we hypothesize that areas with more restaurants are likely to have higher sales price values. 

```{r get open_phily_data, results = "hide"}
planning_districts <- st_read("https://opendata.arcgis.com/datasets/0960ea0f38f44146bb562f2b212075aa_0.geojson")%>%
  st_transform('EPSG:2272')

redevelopment_areas <- st_read("https://data-phl.opendata.arcgis.com/datasets/80f2c71305f5493c8e0aab9137354844_0.geojson") %>%
  dplyr::filter(TYPE == 'Redevelopment Area Plan and Blight Certification' & STATUS == 'Active') %>%
  st_transform('EPSG:2272')

restaurants <- st_read('https://opendata.arcgis.com/datasets/53b8a1c653a74c92b2de23a5d7bf04a0_0.geojson')%>%
  st_transform('EPSG:2272') %>%
  dplyr::select(GEOID10,TOTAL_RESTAURANTS)

restaurants <- restaurants %>%
  mutate(rest_per_sqmi = as.numeric(TOTAL_RESTAURANTS / (st_area(restaurants)/358700000)))

```

## Load School Test Score Data

The proximity to the best public schools is another key characteristic of a desirable neighborhood. We used data on student performance on the PSSA/Keystone test to help provide an understanding of school performance. The PSSA/Keystone includes Science, Math, and English Language Arts score. Students in Grades 3-8 complete this test, and each student receives a score of "Below Basic", "Basic", "Proficient" or "Advanced" on each section. Our analysis uses the percent of students who scored "Advanced" or "Proficient" at each school as a model input. 

In order to get one value to input into our model, the percentage values on the three sections are averaged together get a single number for each school. As an example, if School A had 20 percent of students score proficient or advanced in math, 25 percent score proficient or advanced in sciences, and 35 percent scored proficient or advanced in English a value of 26.66 percent would be calculated as the net performance across all three sections. 26.66 is equal to average of 20, 25, and 35 percent.

Our model assumes that residences are likely to send their students to the public school which is closest to them. Thus, the test scores of the closest school to a house are assigned to the home. 

```{r school_data, cache=FALSE, results = "hide", message=FALSE}
school_test_data <- read.csv('Data\\Schools\\2022 PSSA Keystone Actual (School_S).csv')
schools <- read.csv('Data\\Schools\\2022-2023 Master School List (20230110).csv')

schools <- schools %>%
  rename(ulcs_code = ULCS.Code) %>% 
  separate(col=GPS.Location, into=c('Lat', 'Long'), sep=', ') %>% 
  st_as_sf(coords = c("Long", "Lat"), crs = "EPSG:4326") %>%
  dplyr::select(ulcs_code, School.Name..ULCS., School.Level, Admission.Type)

school_test_data <- school_test_data %>%
  dplyr::filter(category == 'All' & grade == 'Grades 3-8' & profadv_score != 's')  %>%
  mutate(profadv_score = as.double(profadv_score))

school_test_data_summ <- school_test_data %>% 
  group_by(ulcs_code, publicationname) %>% summarize(mean_profadv = mean(profadv_score))

schools <- schools %>%
  left_join(school_test_data_summ, by='ulcs_code') %>%
  drop_na() %>%
  st_transform('EPSG:2272')

```

## Get Data on number of Trees from Open Data Philly

Trees help provide environmental benefits and shade, and residents may place value on having greenness in their neighborhood. We consider including the number of trees in our model because we hypothesize that greener areas with more trees are likely to have higher home sales values. 

Open Data Philly includes a data layer showing all trees in Philadelphia. We calculate the number of trees per square mile in each census tract and included this as a potential input in our housing sales model. 

``` {r get_process_tree_data, results = "hide"}
#Get Data on trees in Philadelphia
trees <- st_read('https://opendata.arcgis.com/api/v3/datasets/5abe042f2927486891c049cf064338cb_0/downloads/data?format=geojson&spatialRefId=4326&where=1%3D1')%>%
  st_transform('EPSG:2272')

#Calculate the number of trees per census tract and convert to trees per square mile to normalize
acsTractsPHL.2021 <- st_intersection(acsTractsPHL.2021, trees) %>% #Determine what census tract each tree is in using intersection
  st_drop_geometry() %>%
  group_by(GEOID) %>% summarize(tree_count = n()) %>% #Count the number of trees in each Census tract
  right_join(acsTractsPHL.2021, by='GEOID') %>% #Join tree cont to census tract Dataframe
  st_sf() %>%
  mutate(trees_per_mi = as.double(tree_count / (st_area(acsTractsPHL.2021)/358700000)))

 #        ggplot()+
#  geom_sf(data=acsTractsPHL.2021, aes(fill=q5(trees_per_mi)))+
# geom_sf(data=planning_districts,fill='transparent',color='red',linewidth=1)
```

## Urban Corridors

Open Data Philly also includes a dataset on urban corridors. We calculated the distance from each home in our home sales dataset to the nearest urban corridor. Living near an urban corridor increases proximity to amenities like restaurants, decreases transportation costs, and improved access to grocery stores. We hypothesize that living near an urban corridor should raise home values. However, there could be exceptions to this pattern. Car-dependent neighborhoods on the outskirts of the city may prefer to be away from corridors, preferring quietness and privacy over proximity to urban corridors. This may result in distance to urban corridors having a non-linear correlation with sales price.

```{r corridors data}
corridors_url <- "https://opendata.arcgis.com/datasets/f43e5f92d34e41249e7a11f269792d11_0.geojson"
corridors <- st_read(corridors_url, quiet= TRUE) %>% st_transform('EPSG:2272')

nearest_fts <- st_nearest_feature(house_data,corridors)

house_data$dist_urb_corridor <- as.double(st_distance(house_data, corridors[nearest_fts,], by_element=TRUE))

```

## Shootings

We calculate the number of shootings within a 1/2 mile and 1/4 mile radius of each home. Shootings can indicate home values because of their impact on the perception of crime in the area and the buyer's concern for personal safety or property damage. Our analysis only includes shootings which have taken placce in 2023, we focus on shootings in 2023 because recent shootings are likely to be the primary driver of current perceptions of crime. 

The shootings data was also obtained through Open Data Philly, the data on shootings is part of a larger crime database which the city of Philadelphia stores on Carto.

```{r homicides data, results = "hide"}
shootings_url <- "https://phl.carto.com/api/v2/sql?q=SELECT+*+FROM+shootings&filename=shootings&format=geojson&skipfields=cartodb_id"
shootings <- st_read(shootings_url) %>% st_transform('EPSG:2272') %>%
  mutate(date_=as.Date(date_, format = "%d-%m-%Y"))  %>%
  dplyr::filter(date_ > '2023-01-01') %>%
  dplyr::select(location)

house_data <- st_intersection(shootings,st_buffer(house_data,2640)) %>%
  st_drop_geometry() %>%
  count(objectid) %>%
  rename(shooting_halfmile = n) %>%
  right_join(house_data, by='objectid') %>%
  mutate(shooting_halfmile = ifelse(is.na(shooting_halfmile),0,shooting_halfmile)) %>%
  st_sf()

house_data <- st_intersection(shootings,st_buffer(house_data,2640/2)) %>%
  st_drop_geometry() %>%
  count(objectid) %>%
  rename(shooting_quartermile = n) %>%
  right_join(house_data, by='objectid') %>%
  mutate(shooting_quartermile = ifelse(is.na(shooting_quartermile),0,shooting_quartermile)) %>%
  st_sf()
```

# Join All Data to House Sales Data

After gathering data from various sources, we now have to coalesce them into one final database for our prediction model. 

```{r Joining All Tables}
HDJoin <- st_intersection(house_data, restaurants %>%
                               dplyr::select("TOTAL_RESTAURANTS", "rest_per_sqmi"))
HDJoin <- st_intersection(HDJoin, planning_districts %>%
                             dplyr::select("DIST_NAME"))

HDJoin <- st_join(HDJoin,schools %>% dplyr::select('mean_profadv'), join=st_nearest_feature)

HDJoinRDAs <- HDJoin[st_intersects(HDJoin, redevelopment_areas) %>% lengths > 0, ] %>% 
  mutate(DevelopmentArea = 1)
  
NotRDAs <- HDJoin[!st_intersects(HDJoin, redevelopment_areas) %>% lengths > 0, ] %>% 
  mutate(DevelopmentArea = 0)

HDBind <- rbind(NotRDAs, HDJoinRDAs)

HDFinal_alldata <- st_intersection(HDBind, acsTractsPHL.2021) 

HDFinal <- HDFinal_alldata %>%
  dplyr::filter(toPredict == "MODELLING")
                           

```

# Summary Statistics 

The table  of Summary Statistics provides us a basis on the range of values and statistical information about each variable we have in our database. With all these factors considered, we can collectively use these variables to predict home prices and provide a visualization of how related specific variables are at indicating such. 

```{r Summary Statistics}
HDFinal_nongeom <- HDFinal %>% st_drop_geometry()
HDFinal_nongeom <- HDFinal_nongeom %>%
  dplyr::select_if(is.numeric) %>%
  dplyr::select(-objectid, -totalPop, -year_built, -totalHU, -number_of_rooms, -TotalOccH)

stargazer(HDFinal_nongeom, type = 'text', title= "Table 1: Summary Statistics")
```


# Correlation Matrix

A correlation matrix compares factors against one another to see how related they are at determining the others value. Each box presented can indicate whether there is a correlation (directly or inverse) or little relationship between them. We are able to see strong correlation between many factors. We are interested in all variables that show a significant relationship with sale price, specifically taking those with an absolute value of 0.3 or greater. 

```{r correlation_matrix, fig.width=12.5, fig.height=20, results = "hide", message=FALSE}
HDFinal_nongeom %>% 
  correlate() %>% 
  autoplot() +
  geom_text(aes(label = round(r,digits=2)), size = 3.5, order = "hclust", type = "upper", tl.cex = 3)

```
# Home Price Scatterplots 

The four graphs below represent four key dependent variables that broadly cover each aspect of a neighborhood in order to determine house prices. Assisted Income focuses on the homeowner's socioeconomic status, resulting in an inverse relationship. Tree count and total livable area are directly correlated with a house's desirability because of home size being considered a luxury and more trees often indicates better care for a neighborhood. The last dependent variable we look at is the white homeowner rate. White homeownership can indicate a "perceived" higher price value because of racial segregation, wealth inequality, and potential gentrification. 


```{r dependent variable correlation, fig.height=6, fig.width=15, results = "hide"}
## Variables: total_livable_area, tree_count, WhiteHomeowner, AssistedIncRate
HDFinal_nongeom %>%
  dplyr::select(sale_price, total_livable_area, tree_count, WhiteHOrate, AssistedIncRate) %>%
  filter(sale_price <= 1000000) %>%
  gather(Variable, Value, -sale_price) %>% 
   ggplot(aes(Value, sale_price)) +
     geom_point(size = .5) + geom_smooth(method = "lm", se=F, colour = "#FA7800") +
     facet_wrap(~Variable, nrow = 1, scales = "free") +
     labs(title = "Price as a function of continuous variables") +
     plotTheme()
```

# Map of Home Prices in Philadelphia

This map explores the variation in 2021 home prices across Philadelphia. We noticed the most expensive areas are located near Center City or far outskirts such as Chestnut Hill. We notice a ring-like effect in the city  where the most expensive areas are either in the core center where all the activity is, or furthest away where there is the least interaction with the city. 

```{r Map of Home Prices, results = "hide"}
HDFinal <- HDFinal %>% 
  mutate(sale_class = cut(sale_price, breaks = c(0, 250000, 500000, 750000, 1000000, max(HDFinal$sale_price, na.rm=TRUE))))

ggplot()+
    geom_sf(data=planning_districts,fill='grey80',color='transparent')+
    geom_sf(data=HDFinal, size=0.75,aes(colour = q5(sale_class)))+
    geom_sf(data=planning_districts,fill='transparent',color='black')+
  scale_color_manual(values = palette5,
                    name = "Sales Price (USD)",
                    na.value = 'grey80',
                    labels = c('$0-$250k', '$250k-$500k', '$500k-$750k', '$750k-$1m', '$1m+'))+
  mapTheme()
    
```

# Interesting Maps 

## Assisted Income

This map shows the rate of people requiring assisted income in the city. We can observe that the central northeast, areas such as Kensington, Fairhill, and Feltonville are most reliant on government assistance for income. And the surrounding area may be affected by the poverty that is prominent there through perimeters that forms, surrounding the "yellow" core.  
 
```{r 3 Interesting Maps Part 1}
ggplot()+
  geom_sf(data=planning_districts,fill='grey80',color='transparent')+
  geom_sf(data=HDFinal,aes(colour = (AssistedIncRate * 100)),size=0.5)+
  scale_color_viridis(name = "Assisted Income Per 100 Households")+
  geom_sf(data=planning_districts,fill='transparent',color='black')+
mapTheme() 
```

## Student Performance by School 

The school system in a neighborhood can be a deal-breaker factor for families seeking a home in an area. The Keystone Exam is the State's index of evaluating school performance and student's education quality. Excellent schools are mostly found on the outskirts. 

```{r Interesting Maps Part 2}
#profadv is the percentage of students recieved proficient OR advanced scores on Keystore Exam by Penn DOE
ggplot()+
  geom_sf(data=planning_districts,fill='grey80',color='transparent')+
  geom_sf(data=schools,aes(colour = q5(mean_profadv)),size=2.5)+
  scale_color_viridis_d(name = "Student Performance on Keystone Exam", labels = c('Poor', 'Below Average', 'Average', 'Above Average', 'Excellent'))+
  geom_sf(data=planning_districts,fill='transparent',color='black')+
mapTheme()
```

## Heat Map of Shootings

We depicted where all shootings have occurred and created a heat map that shows the "hot spot" of where shootings most often occur. Similarly to seeing the education ratings & government assistance, we notice crime is clustered just north of the city center and we it has residual spillover in areas north of said cluster. Central City and areas south of the largest hot spot appear spared from gun violence. 

```{r Interesting Maps Part 3}
ggplot()+
  geom_sf(data=planning_districts,fill='grey80',color='transparent')+
  stat_density2d(data = data.frame(st_coordinates(shootings)), 
                 aes(X, Y, fill = ..level.., alpha = ..level..),
                 size = 0.01, bins = 40, geom = 'polygon') +
  scale_fill_gradient(low = "white", high = "red", name = "Shootings")+
  scale_alpha(guide = "none") +
  labs(title = "Density of Reported Shootings") +
  geom_sf(data=planning_districts,fill='transparent',color='black')+
  mapTheme()
```


# "Something Engaging" Interactive Map of Houseowner-Occupied Rate

Provided below is an map that depicts where the Homeowner occupies the house they reside in, deferring from places where people rent. If you hover above the tract, you'll see the exact rate of home ownership in the area. Other tools are available on the top right of the map. 

```{r Actually something engaging}
ggplotly(
  ggplot(acsTractsPHL.2021)+
  geom_sf(aes(fill = HHOccupiedRate))+
  scale_fill_gradient(low = "black", high = "yellow", name = "Rate of Home Ownership")
)

```
# Training and Test Sets

## Table of Results (Training)

We have decided on a filtered list of key dependent variables to use to predict home values. Those factors being the following: shootings within 1/2 mile, interior condition, total livable area, neighborhood name, trees in the area, median household income,  exterior condition, if there is a fireplace, white homeownership rate, assisted income rate, restaurants per square mile, and the number of bathrooms.  

```{r Training and Test Set}
inTrain <- createDataPartition(
              y = paste(HDFinal$DIST_NAME), 
              p = .60, list = FALSE)
Philly.training <- HDFinal[inTrain,] 
Philly.test <- HDFinal[-inTrain,]  
 
reg.training <- 
  lm(sale_price ~ ., data = as.data.frame(Philly.training) %>% 
                             dplyr::select(sale_price, shooting_halfmile, interior_condition, total_livable_area, 
                                           DIST_NAME, mean_profadv,
                                           DevelopmentArea, tree_count, medHHInc, 
                                           exterior_condition, fireplaces, WhiteHOrate, AssistedIncRate,
                                           rest_per_sqmi, number_of_bathrooms))
summary(reg.training)

```

## Table of Goodness of Fit

For  this process we are now teaching the model to predict the housing prices. We have provided data points as mentioned prior each with their own set of values correlated to sale price. Now, the model learns that relationship and produces its own set of values when given new data. 

For this table below specifically, you see how far off it is from the actual values it compares to, known as their absolute error. Because we are working with large value commodities (i.e housing), the increment the model is  off by can be quite large. It is equally important to look at the percentage value to see the relative amount it is off by as well. 

```{r Reg Training}
Philly.test <-
  Philly.test %>%
  mutate(Regression = "Baseline Regression",
    sale_price.Predict = predict(reg.training, Philly.test),
         sale_price.Error = sale_price.Predict - sale_price,
         sale_price.AbsError = abs(sale_price.Predict - sale_price),
         sale_price.APE = (abs(sale_price.Predict - sale_price)) / sale_price.Predict)%>%
  filter(sale_price < 5000000)

Philly.test %>% 
  st_drop_geometry() %>%
  summarise(MAE = mean(sale_price.AbsError),
            MAPE = abs(mean(sale_price.APE)*100)) %>%
  kbl(col.name=c('Mean Absolute Error','Mean Absolute Percentage Error')) %>%
  kable_classic()
```

## Cross Validation

Our analysis so far, has focused on a single training and test dataset. In order to cross validate the model and assess its generalizability to new data it is necessary to run the model multiple time using different training data. This analysis uses a process called K-Fold Cross Validation - this method involves splitting the data into 100 different training datasets. In each training dataset has a different set of home sales data points are excluded from the model and these excluded points are used to test the model error.

The scatter plot below shows a histogram of the Mean Absolute Error (MAE) for all 100 of the analyses. The MAE varies between 62,500 Dollars and 100,00 Dollars. The distribution of the Mean Absolute Errors has an approximate normal distribution, and peaks at 75,000 USD. Despite being somewhat inaccurate, the indicates the model is generalizable and provides consistent results when using different training data. 

```{r cross_validation}

fitControl <- trainControl(method = "cv", number = 100)
set.seed(825)

reg.cv <- 
  train(sale_price ~ ., data = st_drop_geometry(HDFinal) %>% 
          dplyr::select(sale_price, shooting_halfmile, interior_condition, total_livable_area, 
                        DIST_NAME, mean_profadv, DevelopmentArea, tree_count, medHHInc, 
                        exterior_condition, fireplaces, WhiteHOrate, AssistedIncRate,
                        rest_per_sqmi, number_of_bathrooms), 
        method = "lm", trControl = fitControl, na.action = na.pass)

ggplot(reg.cv$resample, aes(x=MAE)) + 
  geom_histogram(color='white',fill="orange",bins=100)+
  scale_x_continuous(labels = comma,limits=c(0,150000),breaks=c(25000,50000,75000,100000,125000,150000))+
  scale_y_continuous(breaks=c(0,2,4,6,8,10))

```
## Plot Predicted Prices

Explanation needed

```{r, results = "hide", message=FALSE}

Philly.test %>%
  dplyr::select(sale_price.Predict, sale_price) %>%
    ggplot(aes(sale_price, sale_price.Predict)) +
  geom_point() +
  stat_smooth(aes(sale_price, sale_price), 
             method = "lm", se = FALSE, size = 1, colour="#FA7800") + 
  stat_smooth(aes(sale_price.Predict, sale_price), 
              method = "lm", se = FALSE, size = 1, colour="#25CB10") +
  labs(title="Predicted sale price as a function of observed price",
       subtitle="Orange line represents a perfect prediction; Green line represents prediction") +
  scale_x_continuous(labels = comma)+
  plotTheme()
```


# Spatial Pattern of Errors

## Map of Residual Errors 

```{r Resdiuals Map of Test Set, results="hide", message=FALSE}

Philly.test <- Philly.test %>% 
  mutate(spErrorClass = cut(sale_price.Error, breaks = c(min(Philly.test$sale_price.Error, na.rm=TRUE)-1, -100000, -50000, 50000, 100000, max(Philly.test$sale_price.Error, na.rm=TRUE))))

ggplot()+
    geom_sf(data=planning_districts,fill='grey80',color='transparent')+
    geom_sf(data=Philly.test, size=0.5,aes(colour = spErrorClass))+
    geom_sf(data=planning_districts,fill='transparent',color='black')+
    scale_color_manual(values = palette5,
                    name = "Sale Price Error Margins",
                    na.value = 'grey80',
                    labels = c('Less than -100,000', '-100,000 to -50,000', '-50,000 to 50,000', '50,001 - 100,000','More than 100,000'))+
  mapTheme()

```
  
## Spatial Lag Analysis

Based on the map of the residuals above, we can observe that there appears to be some spatial clustering in the residuals. Next we check, if there is similarity between our predicted values and predicated values of surrounding homes. To check this, we use a spatial lag calculation - spatial lag is the weighted sum of the values neighboring a locations. 

The scatter plot below compares the sales price error for each home in our model to the spatial lag error for sales price - our spatial lag error is calculated based on the average error of the five nearest neighbors. The points in the scatter plot are clustered, this indicates that our model generally predicts similar error for a home and the five homes which surround it. The blue line shows the regression line.


```{r Spatial Lag, results = "hide", message=FALSE}
coords.test <-  st_coordinates(Philly.test) 

neighborList.test <- knn2nb(knearneigh(coords.test, 5))

spatialWeights.test <- nb2listw(neighborList.test, style="W")
 
Philly.test %>% 
  mutate(lagPriceError = lag.listw(spatialWeights.test, sale_price.Error)) %>%
  ggplot()+
  geom_point(aes(x =lagPriceError, y =sale_price.Error))+
  stat_smooth(aes(lagPriceError, sale_price.Error), 
             method = "lm", se = FALSE, size = 1, colour="blue")+
  xlim(-1000000,1000000)+
  ylim(-1000000,1000000)+
  plotTheme()
```
# Morans I

Moran's I is a measurement in statistics that evaluates the tendency for data points with similar values to occur near one another. Similar to Nearest Neighbor to statistically prove that houses spatially close to each other would have similar sale prices. This type of measurement helps us identify high and low value clusters (i.e different neighborhoods and their perceived value). 

In the graph below see a thin distribution in gray, those are the counts of houses and their sale prices we provided the regression model. The orange line represents the Moran's I value, determining if there is any correlation between value and location. The observed Moran's I is roughly 0.2, which is considered statistically significant and suggests that the spatial distribution  is correlated. 

```{r "Moran's I"}

moranTest <- moran.mc(Philly.test$sale_price.Error, 
                      spatialWeights.test, nsim = 999)

ggplot(as.data.frame(moranTest$res[c(1:999)]), aes(moranTest$res[c(1:999)])) +
  geom_histogram(binwidth = 0.01) +
  geom_vline(aes(xintercept = moranTest$statistic), colour = "orange",size=1) +
  scale_x_continuous(limits = c(-1, 1)) +
  labs(title="Observed and permuted Moran's I",
       subtitle= "Observed Moran's I in orange",
       x="Moran's I",
       y="Count") +
  plotTheme()
```
# Predicted values for where ‘toPredict’ is both “MODELLING” and “CHALLENGE”.

Explain map 

```{r}

HDFinal_alldata <- HDFinal_alldata %>% 
  mutate(sale_price.Predict = predict(reg.training, HDFinal_alldata))

  
 ggplot()+
   geom_sf(data=planning_districts,fill='grey80',color='transparent')+
   geom_sf(data=HDFinal_alldata, size = 1, aes(color= q5(sale_price.Predict)))+
   facet_wrap(~toPredict)+
   scale_color_viridis_d(labels=qBr(HDFinal_alldata,"sale_price.Predict"),
                   name="Quintile\nBreaks") +
 mapTheme()


```



```{r kable table}
Philly.test %>%
as.data.frame() %>%
  group_by(DIST_NAME) %>%
    summarize(meanPrediction = mean(sale_price.Predict),
              meanPrice = mean(sale_price)) %>%
      kable() %>% 
  kable_styling()
```

# Spatial Pattern of Regression by Neirbodhood (reword for one map)

We can continue to see that there is a correlation between neighborhoods categories and a home's sale price. This spatial autocorrelation is defined by these two maps that show the difference the neighborhood effect has on predictions. When depicted by district, the map  informed us that the baseline model becomes less reliable in the north central area, specifically where there is the lowest values in housing as seen in prior figures. 

```{r}
st_drop_geometry(Philly.test) %>%
  group_by(Regression, DIST_NAME) %>%
  summarize(mean.MAPE = mean(sale_price.APE, na.rm = T)) %>%
  ungroup() %>% 
  left_join(planning_districts) %>%
    st_sf() %>%
    ggplot() + 
      geom_sf(aes(fill = mean.MAPE)) +
      scale_fill_gradient(low = palette5[1], high = palette5[5],
                          name = "Mean Absolute Percent Error") +
      labs(title = "Mean test set MAPE by neighborhood") +
      mapTheme()

```
# Provide a scatterplot plot of MAPE by neighborhood as a function of mean price by neighborhood.

Explain scatterlot

```{r MAPE Scatter Plot}

st_drop_geometry(Philly.test) %>%
  group_by(Regression, DIST_NAME) %>%
  summarize(mean.MAPE = mean(sale_price.APE, na.rm = T), 
            mean.sale_price = mean(sale_price, na.rm = T)) %>%
  ungroup() %>% 
  left_join(planning_districts) %>% 
  ggplot() +
  geom_point(aes(x =mean.sale_price, y =mean.MAPE, color=DIST_NAME))+
  stat_smooth(aes(mean.sale_price, mean.MAPE), 
             method = "lm", se = FALSE, size = .5, colour="red")  
  plotTheme()

```

# Income and Race
These two maps show the significant income gap between races in Philadelphia. There is an evident parallel between higher income areas and fewer minority populations. There are many reasons historically, politically, and through socioeconomic that create and define these boundaries. But that is also exactly why incorporating race as a factor in the models prior were not recommended. 

Adding race to a model adds a cascading effect of discrimination that feeds future predictions to assume race is a pre-determined factor that effects house values. Race is only a factor in housing models because of historical systemic racism and discriminatory policies that created parallels between racial and economic disparity. 

```{r Income and Race, fig.width=10}
grid.arrange(ncol = 2,
  ggplot() + 
    geom_sf(data=planning_districts,fill='beige',color='transparent')+
    geom_sf(data = na.omit(acsTractsPHL.2021), aes(fill = WhiteHOrate)) +
  scale_fill_gradient(low = "black", high = "white", name = "Percentage of White Homeowners")+
    labs(title = "Race Context") +
    mapTheme() + theme(legend.position="bottom"), 
  ggplot() + 
    geom_sf(data=planning_districts,fill='beige',color='transparent')+
    geom_sf(data = na.omit(acsTractsPHL.2021), aes(fill = medHHInc)) +
 scale_fill_gradient(low = "black", high = "green", name = "Median Household Income",
                    na.value = 'grey80',
                    labels = c('$0', '$50k', '$100k', '$150k', '$200k'))+
    labs(title = "Income Context") +
    mapTheme() + theme(legend.position="bottom"))

```

```{r}
acsTractsPHL.2021 <- acsTractsPHL.2021 %>%
 mutate(raceContext = ifelse(WhiteHOrate > .5, "Majority White", "Majority Non-White"),
         incomeContext = ifelse(medHHInc > 52650, "High Income", "Low Income"))

st_join(Philly.test, acsTractsPHL.2021) %>% 
  filter(!is.na(incomeContext)) %>%
  group_by(raceContext, incomeContext) %>%
  summarize(mean.MAPE = scales::percent(mean(sale_price.APE, na.rm = T))) %>%
  st_drop_geometry() %>%
  spread(incomeContext, mean.MAPE) %>%
  kable(caption = "Test set MAPE by neighborhood income context")

```


# Accuracy  & Generalizability (look at it)

Is it accurate but not generalizable as there were specific metrics used for Philadelphia. Any scalability for other cities would require research on their school systems, crime statistics, and other external sources. For accuracy, the primary factor involved is the quality and quantity of data. More data leads to more accurate predictors but in terms of efficiency, quality data that pinpoints crucial values in home price can streamline model training. For the generalization of the model, the model can be generalized for the Philadelphia region alone. Adding data from anywhere outside the region would be inaccurate due to the unfamiliarity of the local context and sets of new data that would be required. 

# Conclusion 

Overall we do not recommend the model because of the racial bias it includes which can create disadvantages to marginalized communities. While the model may be quite accurate, it is crucial to consider the ethical implications of its use. Because of the racial profiling, it is assigning risk to people, and focuses heavily on the homeowner's characteristics rather than the home itself. This process directly leads to discrimination, especially because the model is learning socioeconomic results based on historical discriminatory policy. Ultimately, if your sole priority is to learn what the next sale price is, this model will successfully fulfill that. If one does not want to perpetuate existing inequalities and ethically focus on a righteous model that has an objective learning procedure, we must seek alternative solutions. 