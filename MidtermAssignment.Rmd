---
title: "Midterm_Kapuvari"
author: "Richard Barad and Trevor Kapuvari"
date: "2023-09-25"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
    number_sections: yes
editor_options: 
  markdown: 
    wrap: 72
---

## Introduction

In response to the growing demand by Zillow for a housing market model that incorporates local context, we have been tasked with leverage data from multiple sources, including Philadelphia Open Data, U.S Census Bureau, and the American Community Survey, to gather comprehensive information that enables us to develop an accurate and reliable model for the Philadelphia housing market. 

This report outlines our approach and methods and the importance of understand the local context that is integrated with pre-existing data. We aim to provide greater context and a precise understanding of Philadelphia's housing scene for present and future use. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(sf)
library(spdep)
library(caret)
library(ckanr)
library(FNN)
library(viridis)
library(grid)
library(gridExtra)
library(ggcorrplot) # plot correlation plot
library(corrr)      # another way to plot correlation plot
library(kableExtra)
library(jtools)     # for regression model plots
library(ggstance) # to support jtools plots
library(ggpubr)    # plotting R^2 value on ggplot point scatter
library(broom.mixed) # needed for effects plots
library(tidycensus)
library(ggplot2)
library(sp)
library(ggiraph)
library(plotly)
library(stargazer)
palette5 <- c("#ffffcc","#a1dab4","#41b6c4","#2c7fb8","#253494")


source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")
```


# Get Data
In order to develop an enhanced understanding on the housing market, we acquired data that looks beyond the house itself, but the conditions and amenities that surround such, to explain the deeper meaning behind "location". The first source was from the U.S Census Bureau, as a beginning point in obtaining geographic features for visual display, broad context on parcels and neighborhoods, and overall socioeconomic data. 

The next source of data, similar to the U.S Census, was leveraged by Social Explorer. This website takes U.S Census data and creates a more user-friendly and comprehensive database that allowed us to efficiently seek neighborhood-level information that we can use to experiment with our model. The website fostered our ideas of potential indicators of sale price beyond the housing market, rather amenities surrounding it. This exploration further cascaded to investigate less conventional attributes to a neighborhood that can summarize aspects in a specified quantitative measurement. When taking all of these variables into account, we were able to predict with substantial accuracy the prices of various homes throughout Philadelphia. 


## Census Data

We download the following data from the census: ADD LIST

```{r load_key, warning = FALSE, eval = FALSE}
census_api_key("2ad9e737f3d9062836cb46bb568be5467f86d3db", overwrite = TRUE)
```

```{r acs_vars, include=FALSE}

acs_vars <- c("B01001_001E", # ACS total Pop estimate
              "B25001_001E", # Estimate of total housing units
              "B25002_003E", # Number of vacant housing units
              "B19013_001E", # Median HH Income ($)
              "B19058_002E", # Households with Public Assistance Income
              "B25003_002E", # Owner Occupied Housing Units
              "B25006_002E", # Homeowner is White
              "B25006_001E") # Total Occupied Housing Units 
```

```{r get_acs_2021, message=FALSE, warning=FALSE, cache=TRUE, include=FALSE}
acsTractsPHL.2021 <- get_acs(geography = "tract",
                             year = 2021, 
                             variables = acs_vars, 
                             geometry = TRUE, 
                             state = "PA", 
                             county = "Philadelphia", 
                             output = "wide") %>%
  st_transform('EPSG:2272')
```

```{r do_some_dplyr, cache=FALSE, include=FALSE}
acsTractsPHL.2021 <- acsTractsPHL.2021 %>%
  dplyr::select (GEOID, NAME, all_of(acs_vars))

acsTractsPHL.2021 <- acsTractsPHL.2021 %>%
  rename (totalPop = B01001_001E,
          totalHU = B25001_001E,
          totalVacant = B25002_003E,
          medHHInc = B19013_001E,
          HHAssistedInc = B19058_002E,
          OwnerOccH = B25003_002E,
          WhiteHomeowner = B25006_002E,
          TotalOccH = B25006_001E) %>%
  dplyr::filter(totalPop != 0)

acsTractsPHL.2021 <- acsTractsPHL.2021 %>%
  mutate(HHOccupiedRate = ifelse(OwnerOccH==0,0,OwnerOccH/totalHU),
         WhiteHOrate = ifelse(WhiteHomeowner==0,0,WhiteHomeowner/TotalOccH),
         AssistedIncRate = ifelse(HHAssistedInc==0,0,HHAssistedInc/totalHU),
         medHHInc = ifelse(is.na(medHHInc),mean(medHHInc,na.rm=TRUE),medHHInc))
```

## Get Data from Open Data Philly

OpenData Philly is an open source website that provides a catalog of free data, officially sponsored by the City of Philadelphia. The data provided by the City and other organizations allows us to collect a wide variety of information we can utilize to categorize, describe, and develop a profile for each neighborhood. We used this website to gather 

```{r get open_phily_data}
planning_districts <- st_read("https://opendata.arcgis.com/datasets/0960ea0f38f44146bb562f2b212075aa_0.geojson")%>%
  st_transform('EPSG:2272')

redevelopment_areas <- st_read("https://data-phl.opendata.arcgis.com/datasets/80f2c71305f5493c8e0aab9137354844_0.geojson") %>%
  dplyr::filter(TYPE == 'Redevelopment Area Plan and Blight Certification' & STATUS == 'Active') %>%
  st_transform('EPSG:2272')

restaurants <- st_read('https://opendata.arcgis.com/datasets/53b8a1c653a74c92b2de23a5d7bf04a0_0.geojson')%>%
  st_transform('EPSG:2272') %>%
  dplyr::select(GEOID10,TOTAL_RESTAURANTS)

restaurants <- restaurants %>%
  mutate(rest_per_sqmi = as.numeric(TOTAL_RESTAURANTS / (st_area(restaurants)/358700000)))

```

## Load House Sales Data from Github
The data here extracts the samples of house data we were provided prior to the our model and studies. The database and attributes included creates a base-model of indicators to predict house price that focus exclusively on the house's features. Prior data mentioned focuses on the external factors while this data focuses on the internal. The attributes we look for in the house data needs to be quantifiable and relevant. Large sets of data were ranked and categorized which required us to manually set a value that can be calculated by the model.  Beyond the various factors that can be considered for the predictive model, the general table will be filtered and selected to determine only the most important indicators. 

```{r read_github_data}
house_data <- st_read("https://raw.githubusercontent.com/mafichman/musa_5080_2023/main/Midterm/data/2023/studentData.geojson") %>%
  dplyr::select("objectid","central_air","exterior_condition", "fireplaces", "garage_spaces", "interior_condition", "mailing_street", "location", "number_of_bathrooms", "number_of_bedrooms","number_of_rooms","number_stories","sale_price","sale_date","total_area","total_livable_area","year_built","toPredict") %>%
  st_transform('EPSG:2272')

#Do some feature enginerring
house_data <- house_data %>%
  mutate(
    #Recode exterior condition so that four is best condition, group some categories together
    exterior_condition = as.numeric(recode(exterior_condition, '7'='1', '6'='2', '5'='3', '4'='3', '3'='4', '2'='4', '1'='4')), 
    #Assumed averager condition is value is NA
    exterior_condition = ifelse(is.na(exterior_condition),2,exterior_condition),
    #Set blank values to 0, assumed that if value is blank there are no fireplaces based on metadata.
    fireplaces = ifelse(is.na(fireplaces),0,fireplaces),
    #Assumed there is always at least one bathroom - if property has 0 bathrooms assigned a value of 1.
    number_of_bathrooms = ifelse(number_of_bathrooms == 0,1,number_of_bathrooms),
    number_of_bathrooms = ifelse(is.na(number_of_bathrooms),1,number_of_bathrooms),
    #Assumed averager condition is value is NA
    interior_condition = ifelse(is.na(interior_condition),4,interior_condition))
  

```

## Load School Test Score Data

We look at the percent of students at public schools who had proficient or advanced scores on the PSSA/Keystone test. The PSSA/Keystone includes Science, Math, and English Language Arts score. For each school, we averaged together the students performance on the three sections to get a single number for each school. We then determined the nearest school to each home and assigned the home the test score value of that school.

```{r school_data, cache=FALSE, include=FALSE}
school_test_data <- read.csv('Data\\Schools\\2022 PSSA Keystone Actual (School_S).csv')
schools <- read.csv('Data\\Schools\\2022-2023 Master School List (20230110).csv')

schools <- schools %>%
  rename(ulcs_code = ULCS.Code) %>% 
  separate(col=GPS.Location, into=c('Lat', 'Long'), sep=', ') %>% 
  st_as_sf(coords = c("Long", "Lat"), crs = "EPSG:4326") %>%
  dplyr::select(ulcs_code, School.Name..ULCS., School.Level, Admission.Type)

school_test_data <- school_test_data %>%
  dplyr::filter(category == 'All' & grade == 'Grades 3-8' & profadv_score != 's')  %>%
  mutate(profadv_score = as.double(profadv_score))

school_test_data_summ <- school_test_data %>% 
  group_by(ulcs_code, publicationname) %>% summarize(mean_profadv = mean(profadv_score))

schools <- schools %>%
  left_join(school_test_data_summ, by='ulcs_code') %>%
  drop_na() %>%
  st_transform('EPSG:2272')

```

## Get Data on number of Trees from Open Data Philly

We download data on trees as points and calculate the number of trees per square mile in each census tract. Trees are a way of indicating aesthetic appeal, environmental benefits, and shade. We extracted this data from OpenDataPhilly and joined the data from such to the census tracts.    

``` {r get_process_tree_data}
#Get Data on trees in Philadelphia
trees <- st_read('https://opendata.arcgis.com/api/v3/datasets/5abe042f2927486891c049cf064338cb_0/downloads/data?format=geojson&spatialRefId=4326&where=1%3D1')%>%
  st_transform('EPSG:2272')

#Calculate the number of trees per census tract and convert to trees per square mile to normalize
acsTractsPHL.2021 <- st_intersection(acsTractsPHL.2021, trees) %>% #Determine what census tract each tree is in using intersection
  st_drop_geometry() %>%
  group_by(GEOID) %>% summarize(tree_count = n()) %>% #Count the number of trees in each Census tract
  right_join(acsTractsPHL.2021, by='GEOID') %>% #Join tree cont to census tract Dataframe
  st_sf() %>%
  mutate(trees_per_mi = as.double(tree_count / (st_area(acsTractsPHL.2021)/358700000)))

 #        ggplot()+
#  geom_sf(data=acsTractsPHL.2021, aes(fill=q5(trees_per_mi)))+
# geom_sf(data=planning_districts,fill='transparent',color='red',linewidth=1)
```

## Urban Corridors

We calculate the distance to the nearest urban corridor. Urban corridors, for most houses, are considered proximity to amenities, decrease transportation costs, and has an overall higher quality of life. It has to also be noted that outliers are present in suburban housing too. For an urban development in general Philadelphia, being close to urban corridors would be considered an amenities, raising the home value. For subruban areas on the outskirts of the city, these car-dependent neighborhoods want to be away from corridors, preferring quietness, privacy, and distance. 

```{r corridors data}
corridors_url <- "https://opendata.arcgis.com/datasets/f43e5f92d34e41249e7a11f269792d11_0.geojson"
corridors <- st_read(corridors_url, quiet= TRUE) %>% st_transform('EPSG:2272')

nearest_fts <- st_nearest_feature(house_data,corridors)

house_data$dist_urb_corridor <- as.double(st_distance(house_data, corridors[nearest_fts,], by_element=TRUE))

```

## Shootings

We calculate the number of shootings within a 1/2 mile and 1/4 mile radius of each home.

```{r homicides data}
shootings_url <- "https://phl.carto.com/api/v2/sql?q=SELECT+*+FROM+shootings&filename=shootings&format=geojson&skipfields=cartodb_id"
shootings <- st_read(shootings_url) %>% st_transform('EPSG:2272') %>%
  mutate(date_=as.Date(date_, format = "%d-%m-%Y"))  %>%
  dplyr::filter(date_ > '2023-01-01') %>%
  dplyr::select(location)

house_data <- st_intersection(shootings,st_buffer(house_data,2640)) %>%
  st_drop_geometry() %>%
  count(objectid) %>%
  rename(shooting_halfmile = n) %>%
  right_join(house_data, by='objectid') %>%
  mutate(shooting_halfmile = ifelse(is.na(shooting_halfmile),0,shooting_halfmile)) %>%
  st_sf()

house_data <- st_intersection(shootings,st_buffer(house_data,2640/2)) %>%
  st_drop_geometry() %>%
  count(objectid) %>%
  rename(shooting_quartermile = n) %>%
  right_join(house_data, by='objectid') %>%
  mutate(shooting_quartermile = ifelse(is.na(shooting_quartermile),0,shooting_quartermile)) %>%
  st_sf()
```

# Join All Data to House Sales Data

```{r Joining All Tables}
HDJoin <- st_intersection(house_data, restaurants %>%
                               dplyr::select("TOTAL_RESTAURANTS", "rest_per_sqmi"))
HDJoin <- st_intersection(HDJoin, planning_districts %>%
                             dplyr::select("DIST_NAME"))

HDJoin <- st_join(HDJoin,schools %>% dplyr::select('mean_profadv'), join=st_nearest_feature)

HDJoinRDAs <- HDJoin[st_intersects(HDJoin, redevelopment_areas) %>% lengths > 0, ] %>% 
  mutate(DevelopmentArea = 1)
  
NotRDAs <- HDJoin[!st_intersects(HDJoin, redevelopment_areas) %>% lengths > 0, ] %>% 
  mutate(DevelopmentArea = 0)

HDBind <- rbind(NotRDAs, HDJoinRDAs)

HDFinal <- st_intersection(HDBind, acsTractsPHL.2021 %>%
                             dplyr::select(-NAME)) %>%
                             dplyr::filter(toPredict == "MODELLING")
                           

```


``` {r make_maps (TO DELETE LATER)}



C <- ggplot()+
  geom_sf(data=restaurants,aes(fill = q5(rest_per_mi)))

D <- ggplot()+
  geom_sf(data=restaurants,aes(fill = q5(TOTAL_RESTAURANTS)))
```

```{r Summary Statistics}
HDFinal_nongeom <- HDFinal %>% st_drop_geometry()
HDFinal_nongeom <- HDFinal_nongeom %>%
  dplyr::select_if(is.numeric) %>%
  dplyr::select(-objectid, -totalPop, -year_built, -totalHU, -number_of_rooms, -TotalOccH)

stargazer(HDFinal_nongeom, type = 'text', title= "Table 1: Summary Statistics")
```
```{r correlation_matrix, fig.height=6.5, fig.width=17.5}
HDFinal_nongeom %>% 
  correlate() %>% 
  autoplot() +
  geom_text(aes(label = round(r,digits=2)), size = 2, order = "hclust", type = "upper", tl.cex = 0.7)

```


```{r dependent variable correlation, fig.height=6, fig.width=10}
## Variables: total_livable_area, tree_count, WhiteHomeowner, AssistedIncRate
HDFinal_nongeom %>%
  dplyr::select(sale_price, total_livable_area, tree_count, WhiteHomeowner, AssistedIncRate) %>%
  filter(sale_price <= 1000000) %>%
  gather(Variable, Value, -sale_price) %>% 
   ggplot(aes(Value, sale_price)) +
     geom_point(size = .5) + geom_smooth(method = "lm", se=F, colour = "#FA7800") +
     facet_wrap(~Variable, nrow = 1, scales = "free") +
     labs(title = "Price as a function of continuous variables") +
     plotTheme()
```
```{r Map of Home Prices}
HDFinal <- HDFinal %>% 
  mutate(sale_class = cut(sale_price, breaks = c(0, 250000, 500000, 750000, 1000000, max(HDFinal$sale_price, na.rm=TRUE))))

ggplot()+
    geom_sf(data=planning_districts,fill='grey80',color='transparent')+
    geom_sf(data=HDFinal, size=0.75,aes(colour = (sale_class)))+
    geom_sf(data=planning_districts,fill='transparent',color='black')+
  scale_color_manual(values = palette5,
                    name = "Sales Price (USD)",
                    na.value = 'grey80',
                    labels = c('$0-$250k', '$250k-$500k', '$500k-$750k', '$750k-$1m', '$1m+'))+
  mapTheme()
    
```

```{r 3 Interesting Maps Part 1}
ggplot()+
  geom_sf(data=planning_districts,fill='grey80',color='transparent')+
  geom_sf(data=HDFinal,aes(colour = (AssistedIncRate * 100)),size=0.5)+
  scale_color_viridis(name = "Assisted Income Per 100 Households")+
  geom_sf(data=planning_districts,fill='transparent',color='black')+
mapTheme()
```

```{r Interesting Maps Part 2}
#profadv is the percentage of students recieved proficient OR advanced scores on Keystore Exam by Penn DOE
ggplot()+
  geom_sf(data=planning_districts,fill='grey80',color='transparent')+
  geom_sf(data=schools,aes(colour = q5(mean_profadv)),size=2.5)+
  scale_color_viridis_d(name = "Student Performance on Keystone Exam", labels = c('Poor', 'Below Average', 'Average', 'Above Average', 'Excellent'))+
  geom_sf(data=planning_districts,fill='transparent',color='black')+
mapTheme()
```

```{r Interesting Maps Part 3}
ggplot()+
  geom_sf(data=planning_districts,fill='grey80',color='transparent')+
  stat_density2d(data = data.frame(st_coordinates(shootings)), 
                 aes(X, Y, fill = ..level.., alpha = ..level..),
                 size = 0.01, bins = 40, geom = 'polygon') +
  scale_fill_gradient(low = "white", high = "red", name = "Shootings")+
  scale_alpha(guide = "none") +
  labs(title = "Density of Reported Shootings") +
  geom_sf(data=planning_districts,fill='transparent',color='black')+
  mapTheme()
```
```{r "Something Engaging"}
ggplot()+
  geom_sf(data=planning_districts,fill='grey90',color='transparent')+
  geom_sf(data=HDFinal,aes(colour = q5(sale_class)),size=0.5)+
  geom_sf(data=redevelopment_areas,fill='transparent',color='black')+
  scale_color_manual(values = palette5,
                    name = "Sales Price (USD)",
                    na.value = 'grey80',
                    labels = c('$0-$250k', '$250k-$500k', '$500k-$750k', '$750k-$1m', '$1m+'))+
  mapTheme()

```
```{r Actually something engaging}
ggplotly(
  ggplot(restaurants)+
  geom_sf(aes(fill = TOTAL_RESTAURANTS))+
  scale_fill_gradient(low = "black", high = "yellow", name = "Total Restaurants")
)

```

```{r Training and Test Set}
inTrain <- createDataPartition(
              y = paste(HDFinal$DIST_NAME), 
              p = .60, list = FALSE)
Philly.training <- HDFinal[inTrain,] 
Philly.test <- HDFinal[-inTrain,]  
 
reg.training <- 
  lm(sale_price ~ ., data = as.data.frame(Philly.training) %>% 
                             dplyr::select(sale_price, shooting_halfmile, interior_condition, total_livable_area, 
                                           DIST_NAME, mean_profadv,
                                           DevelopmentArea, tree_count, medHHInc, 
                                           exterior_condition, fireplaces, WhiteHOrate, AssistedIncRate,
                                           rest_per_sqmi, number_of_bathrooms))
summary(reg.training)
#stargazer(reg.training, type = 'text', title= "Regression Training Results")

```

```{r Reg Training}
Philly.test <-
  Philly.test %>%
  mutate(sale_price.Predict = predict(reg.training, Philly.test),
         sale_price.Error = sale_price.Predict - sale_price,
         sale_price.AbsError = abs(sale_price.Predict - sale_price),
         sale_price.APE = (abs(sale_price.Predict - sale_price)) / sale_price.Predict)%>%
  filter(sale_price < 5000000)

Philly.test %>% 
  st_drop_geometry() %>%
  summarise(MAE = mean(sale_price.AbsError),
            MAPE = mean(sale_price.APE)*100) %>%
  kbl(col.name=c('Mean Absolute Error','Mean Absolute Percentage Error')) %>%
  kable_classic()
```